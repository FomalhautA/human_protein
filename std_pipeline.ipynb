{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convolve\n",
    "conv = tf.contrib.layers.conv2d(inputs,\n",
    "                                num_outputs,\n",
    "                                kernel_size=, \n",
    "                                stride,\n",
    "                                padding,\n",
    "                                data_format,\n",
    "                                rate,\n",
    "                                activation_fn,\n",
    "                                normalizer_fn,\n",
    "                                normalizer_params,\n",
    "                                weights_initializer,\n",
    "                                weights_regularizer,\n",
    "                                biases_initializer,\n",
    "                                biases_regularizer,\n",
    "                                reuse,\n",
    "                                variables_collections,\n",
    "                                outputs_collections,\n",
    "                                trainable,\n",
    "                                scope)\n",
    "\n",
    "# Pooling\n",
    "pl = tf.contrib.layers.max_pool2d(inputs,\n",
    "                                  kernel_size,\n",
    "                                  stride=2,\n",
    "                                  padding=\"VALID\",\n",
    "                                  data_format=DATA_FORMAT_NHWC, \n",
    "                                  outputs_collections=None,\n",
    "                                  scope=None)\n",
    "\n",
    "# Full connect\n",
    "fc = tf.contrib.layers.fully_connected(inputs,\n",
    "                                       1024,\n",
    "                                       scope='fc_layer')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(x, ):\n",
    "    weights = tf.get_variable(\"weights\",\n",
    "                              shape, \n",
    "                              initializer=\n",
    "                              init_ops.random_normal_initializer(init_mean,\n",
    "                                                                 init_stddev,\n",
    "                                                                 dtype=dtype),\n",
    "                              dtype=dtype)\n",
    "    biases = tf.get_varaible(\"bias\",\n",
    "                             shape,\n",
    "                             initializer=\n",
    "                             init_ops.random_normal_initializer(init_mean,\n",
    "                                                                init_stddev,\n",
    "                                                                dtype=dtype),\n",
    "                             dtype=dtype)\n",
    "\n",
    "    logits = tf.nn.xw_plus_b(x, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits, onehot_labels,\n",
    "                                               weights=, \n",
    "                                               label_smoothing=0, \n",
    "                                               scope=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_op = tf.contrib.layers.optimize_loss(loss, global_step, \n",
    "                                           learning_rate, optimizer=,\n",
    "                                           gradient_noise_scale=, \n",
    "                                           gradient_multipliers=, \n",
    "                                           clip_gradients=,\n",
    "                                           learning_rate_decay_fn=, \n",
    "                                           update_ops=, varialbes=, \n",
    "                                           name=, summaries=,\n",
    "                                           colocate_gradients_with_ops=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(features, labels, mode):\n",
    "    inputs = tf.placeholder(tf.float32, [None, height, weight, connels])\n",
    "    labels = tf.placeholder(tf.float32, [None, ])\n",
    "    \n",
    "    #conv1\n",
    "    filter1 = tf.placeholder(tf.float32, [None, ])\n",
    "    conv1 = tf.nn.conv2d(inputs, filter1, strides, padding)\n",
    "    pool1 = tf.nn.max_pool(conv1, ksize, strides, padding)\n",
    "    norm1 = tf.nn.local_response_normalization(pool1, depth_radius=5,\n",
    "                                               bias=1, alpha=1,\n",
    "                                               beta=0.5)\n",
    "    \n",
    "    #conv2\n",
    "    filter2 = tf.placeholder(tf.float32, [None, ])\n",
    "    conv2 = tf.nn.conv2d(pool1, filter2, strides, padding)\n",
    "    pool2 = tf.nn.max_pool(conv2, ksize, strides, padding)\n",
    "    norm2 = tf.nn.local_response_normalization(pool2, depth_radius=5,\n",
    "                                               bias=1, alpha=1,\n",
    "                                               beta=0.5)\n",
    "    \n",
    "    #fc\n",
    "    local3 = tf.layers.dense(inputs=norm2, units, activation, use_bias,\n",
    "                    kernel_initializer, bias_initializer,\n",
    "                    activity_regularizer, kernel_constraint,\n",
    "                    bias_constraint, trainable, reuse)\n",
    "    \n",
    "    #fc\n",
    "    local4 = tf.layers.dense(inputs=local3, units, activation, use_bias,\n",
    "                             kernel_initializer, bias_initializer,\n",
    "                             activity_regularizer, kernel_constraint,\n",
    "                             bias_constraint, trainable, reuse)\n",
    "    \n",
    "    \n",
    "    \n",
    "    output = tf.nn.softmax(local4)\n",
    "    \n",
    "    #dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = \n",
    "classifier.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = \n",
    "score = \n",
    "print(\"Accuracy: {0: f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = shuffle(X, Y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = \n",
    "train_test_split(X, Y, test_size=0.2, random_state=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
